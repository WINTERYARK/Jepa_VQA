# -*- coding: utf-8 -*-
"""“VQA Dataset.ipynb”的副本

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/141mnheS8oNP5AU2KgvyaLypt_av8W991

# Captions instead of VQA

* Simpler
* Better datasets

## Dataset
"""

import datasets

# There are other datasets that you can use later for more data. We are using validation as it is small.
# dataset: datasets.Dataset = datasets.load_dataset("Multimodal-Fatima/COCO_captions_train", split="train")
# dataset: datasets.Dataset = datasets.load_dataset("Multimodal-Fatima/COCO_captions_validation", split="validation")
dataset: datasets.Dataset = datasets.load_dataset("Multimodal-Fatima/COCO_captions_test", split="test")
dataset

"""## Inspect Sample"""

sample = dataset[0]
sample.keys()

sample

import PIL

image: PIL.Image = sample["image"]
print(image.size) # (640, 360)
image

sentences_raw: list[str] = sample["sentences_raw"]
sentences_raw

"""## Prepocess Image"""

from transformers import AutoImageProcessor

image_processor = AutoImageProcessor.from_pretrained("facebook/dinov2-base")
image_processor

import numpy as np
from transformers import BatchFeature

# Get original size
original_width, original_height = image.size
max_longest_edge = 256

# Compute new size preserving aspect ratio, with longest edge = 256
scale = max_longest_edge / max(original_width, original_height)
new_width = int(round(original_width * scale))
new_height = int(round(original_height * scale))

# Resize image manually using PIL
resized_image = image.resize((new_width, new_height), resample=PIL.Image.BICUBIC)
print(resized_image.size)

processed_image: BatchFeature = image_processor(
    images=resized_image,
    do_resize=False,
    do_center_crop=False,
    do_rescale=True,
    do_normalize=True,
    return_tensors="pt"
)

image_array = (processed_image.pixel_values.squeeze(0).permute(1, 2, 0).numpy() * 255).astype(np.uint8)
print(image_array.shape)

PIL.Image.fromarray(image_array)

"""## Preprocess Text"""

from transformers import AutoTokenizer

text_processor = AutoTokenizer.from_pretrained("facebook/bart-base")
text_processor

from transformers import BatchEncoding

processed_texts: BatchEncoding = text_processor(
    sentences_raw,
    padding=True,
    truncation=True,
    return_tensors="pt"
)
dict(processed_texts).keys()

sample["image"]

"""## DataLoader"""

import torch

class CocoDataset(datasets.Dataset):
    def __init__(
            self,
            dataset: datasets.Dataset,
        ):
        self.dataset = dataset

    def __len__(self):
        return len(self.dataset)

    def __getitem__(self, idx: int):
        sample = self.dataset[idx]

        return {
            "image": sample["image"],
            "text": sample["sentences_raw"]
        }

coco_dataset = CocoDataset(dataset=dataset)

coco_dataset[0]

import random


def process_images(images: list[PIL.Image]) -> torch.Tensor:
    processed: BatchFeature = image_processor(
        images=images,
        size={"height": 224, "width": 224},  # Fixed size recommended for DINO
        do_resize= True,
        do_center_crop= False,
        do_rescale= True,
        do_normalize= True,
        return_tensors="pt"
    )

    return processed.pixel_values  # shape: (B, 3, H, W)

def process_text(sentences_raw: list[str]) -> BatchEncoding:
    processed: BatchEncoding = text_processor(
        sentences_raw,
        padding=True,
        truncation=True,
        return_tensors="pt"
    )

    return processed

def collate_fn(samples) -> dict[str, torch.Tensor]:
    images = [sample["image"] for sample in samples]
    texts = [sample["text"] for sample in samples]

    # Flatten lists of lists (assuming each 'text' is a list of captions)
    flattened_texts = [random.choice(t) if isinstance(t, list) else t for t in texts]

    return {
        "image": process_images(images), # torch.Tensor (B, 3, H, W)
        "text": process_text(flattened_texts) # BatchEncoding
    }

from torch.utils.data import DataLoader

dataloader = DataLoader(
    coco_dataset,
    batch_size=16,
    shuffle=False,
    collate_fn=collate_fn
)

processed: BatchFeature = image_processor(
        images=image,
        size={"height": 224, "width": 224},
        do_resize= True,
        do_center_crop= False,
        do_rescale= True,
        do_normalize= True,
        return_tensors="pt"
    )

for s in dataloader:
    print(s.keys())
    break

s["image"].shape # a batch of images

(
    s["text"].input_ids.shape, # a batch of text token ids
    s["text"].attention_mask.shape # a batch of text attention masks
)

"""## Encode"""

from transformers import AutoModel

image_model = AutoModel.from_pretrained("facebook/dinov2-base")
image_model

s["image"].shape

with torch.no_grad():
    image_embeddings = image_model(s["image"]).last_hidden_state

image_embeddings.shape

from transformers import AutoModel

text_model = AutoModel.from_pretrained("facebook/bart-base")
text_model

text_encoder = text_model.encoder # This is what we pass to JEPA as the text encoder in stage 1
text_encoder

dict(s["text"]).keys()

text_embeddings = text_encoder(**s["text"]).last_hidden_state
text_embeddings.shape

"""# Note the text embeddings (targets) and the image embeddings (context) have the same embed dimension (768). This makes them dimensionally compatible!"""

def random_sample_indices(b: int, t: int, prob: float, device=None) -> torch.Tensor:
    """
    Get random token indices per batch.

    Args:
        b: batch size
        t: sequence length
        prob: fraction of tokens to sample
        device: torch device

    Returns:
        idx: LongTensor of shape (b, n) with sampled indices
    """
    n = max(1, int(round(prob * t)))
    idx = torch.rand(b, t, device=device).argsort(dim=1)[:, :n]  # (b, n)

    return idx

"""Get contexts from images"""

# Randomly sample from image embeddings to make context blocks

B, t, _ = image_embeddings.shape
context_prob: float = 0.7

context_idxs: torch.Tensor = random_sample_indices(b=B, t=t, prob=context_prob, device=image_embeddings.device)
context_idxs.shape

"""Get targets and masks from text"""

import torch.nn as nn

# Create target masks (with positional embeddings)
B, T, D = text_embeddings.shape

mask_token = nn.Parameter(torch.randn(1, 1, D))

mask_token_expanded = mask_token.expand(B, T, D)  # (B, T, D)
positions = text_encoder.embed_positions(mask_token_expanded)  # BartLearnedPositionalEmbedding(1026, 768)

masks_with_positions = mask_token_expanded + positions  # (B, T, D)
masks_with_positions.shape

# Randomly sample from text embeddings to make target blocks (you should use most of the text embeddings)

target_prob: float = 0.9

target_idxs: torch.Tensor = random_sample_indices(b=B, t=T, prob=target_prob, device=text_embeddings.device)
target_idxs.shape

targets: torch.Tensor = text_embeddings[torch.arange(B)[:, None], target_idxs]
targets.shape

target_masks: torch.Tensor = masks_with_positions[torch.arange(B)[:, None], target_idxs]
target_masks.shape

# concat the context and target masks and pass into predictor
# predictor will return predictions of targets
# pass predictions and targets into your loss